---
alwaysApply: true
---
### Guiding Principles for Developing `llm-core`

You are an expert AI programming assistant paired with a human developer to build and enhance the `llm-core` Rust library. Your primary goal is to write clean, robust, and maintainable code that strictly adheres to the project's established architectural patterns.

IMPORTANT: Ensure all PyO3 implementations follow the instructions in 'https://pyo3.rs/v0.23.0/migration' closely.

**Core Architectural Pillars:**

1.  **The `Orchestra` is Central (`orchestra.rs`):**
    *   The `Orchestra` is the **stateless** engine for all AI interactions.
    *   **ALL** AI model calls **must** be routed through an `Orchestra` instance. This ensures that all calls benefit from the built-in provider abstraction, configuration management, retry logic, and strategy selection (e.g., native tools vs. "Lucky" fallback).
    *   Avoid implementing one-off HTTP clients or bypassing the `Orchestra`.

2.  **Provider Abstraction is Key (`providers/`):**
    *   All provider-specific logic is isolated within its own module (e.g., `providers/gemini.rs`).
    *   New provider features (like image generation) or support for new providers **must** be implemented by extending the `ProviderAdapter` and `ResponseParser` traits in `providers/mod.rs`.
    *   Do not bleed provider-specific logic into the `Orchestra` or tools.

3.  **Tools are Self-Contained Wrappers (`config/toolkit.rs`):**
    *   Tools are defined as simple, synchronous Rust functions.
    *   If a tool needs to perform an `async` operation (like calling the `Sorter` or `Orchestra`), it **must** create its own temporary `tokio` runtime to bridge the synchronous and asynchronous contexts.
    *   Tools should be self-contained and rely on the public APIs of other components (`Sorter::run_sorting_task`, `Orchestra::new`, etc.), not their internal implementation details.
    *   The schema for a tool's parameters must be clearly and comprehensively defined in its `FunctionDefinition`.

4.  **Configuration is Centralized (`config.rs`, `models.json`):**
    *   `models.json` is the **single source of truth** for all supported AI models. Any new model must be added there first.
    *   API keys and other secrets are managed via `.env` files and accessed through `config::get_env_var`.

5.  **Testing is End-to-End (`tests/full_test.rs`):**
    *   New features must be accompanied by a robust integration test.
    *   Tests for tool-based features should use the `Chat` struct to simulate a realistic, multi-step conversation.
    *   Assertions against LLM responses should be flexible (e.g., `contains("keyword")`) to avoid failures due to minor variations in wording.
    *   For tests involving file I/O, **always** write to an unambiguous, reliable location like the system's temporary directory (`std::env::temp_dir()`) to prevent environment-specific permission errors.

**Your Workflow for New Features:**

1.  **Analyze the Request:** Understand how the new feature fits into the existing architecture.
2.  **Extend the Interface:** If necessary, add new methods to the `ProviderAdapter` or `Orchestra`.
3.  **Implement the Core Logic:** Write the new functionality, ensuring it follows the principles above.
4.  **Expose as a Tool (if applicable):** Create a simple, synchronous wrapper in `toolkit.rs`.
5.  **Write a Comprehensive Test:** Add a new test case to `full_test.rs` that validates the entire end-to-end flow.

By following these principles, you will ensure the continued quality and stability of the `llm-core` codebase.

***

### `llm-core` Development & Contribution Guidelines

These rules are designed to maintain the architectural integrity and code quality of the project. 

1.  **Provider & Model Integration**
    *   **Encapsulation is Key**: All logic for a new API provider (e.g., a new LLM company) must be contained within its own module in `llm_core/core/src/providers/`.
    *   **Implement Core Traits**: Every new provider must implement the `ProviderAdapter` and `ResponseParser` traits. The `Adapter` is for formatting requests, and the `Parser` is for normalizing responses into the standard `ResponsePayload`.
    *   **Register New Providers**: After creating a provider module, it must be registered in `llm_core/core/src/providers/mod.rs` and its `Adapter`/`Parser` must be mapped by name in `orchestra.rs`.
    *   **Use the Model Catalog**: All new models must be added to `llm_core/core/src/config/models.json`. This file is the single source of truth for model names, API tags, pricing, and capabilities. Do not hardcode this information elsewhere.

2.  **Configuration & Secrets Management**
    *   **No Hardcoded Secrets**: API keys and other secrets must never be hardcoded. They should be loaded from environment variables using the `.env` file standard.
    *   **Follow the `env:` Convention**: When adding a new key or URL to `models.json`, its value must be a reference to an environment variable, prefixed with `env:`, like `"env:MY_API_KEY"`.
    *   **Use the Getter**: Always use the `config::get_env_var` function to retrieve secrets, as it handles the `.env` loading and error reporting.

3.  **Python Bindings and Interoperability**
    *   **Expose via PyO3**: Any Rust functionality intended for Python users must be exposed using PyO3 macros (`#[pyclass]`, `#[pymethods]`, `#[pyfunction]`).
    *   **Centralize Bindings**: All Python binding code should reside in `llm_core/core/src/bindings/python_b.rs`.
    *   **Register in `lib.rs`**: Every new exposed class or function must be added to the module in `llm_core/core/src/lib.rs` to be accessible from Python.
    *   **Standardize Data Conversion**: Use the provided `json_to_pyobject` and `pyobject_to_json` helpers for converting data between Rust (`serde_json::Value`) and Python objects to ensure consistency.
    *   **Manage Async in Rust**: For Python methods that call async Rust functions, use a `tokio` runtime within the `PyChat` struct to `block_on` the future. This keeps the Python interface synchronous and simple.

4.  **Error Handling**
    *   **Centralized Error Type**: All functions within the library should return a `Result` that uses the `LLMCoreError` enum. This provides a single, consistent error type for the entire library.
    *   **Use `From` Implementations**: Leverage the existing `From<T>` implementations in `error.rs` to ergonomically convert external errors (e.g., from `reqwest`, `serde_json`, `std::io`) into `LLMCoreError`.
    *   **Be Specific**: When returning an error, choose the most descriptive variant from `LLMCoreError` (e.g., `ApiError` for HTTP errors, `ResponseParseError` for JSON issues).

5.  **Orchestration and Strategy**
    *   **Stateless `Orchestra`, Stateful `Chat`**: Keep the `Orchestra` struct stateless. It should only be responsible for a single API call transaction. The `Chat` struct in `convo.rs` is responsible for managing state across multiple turns (e.g., conversation history).
    *   **Intelligent Strategy Selection**: When adding support for a new provider, correctly implement `supports_native_schema` and `supports_tools` in its `ProviderAdapter`. The `Orchestra` relies on these methods to decide whether to use a provider's native features or fall back to the `Lucky` prompting strategy.

6.  **Usage & Cost Tracking**
    *   **Log Every Turn**: Any function that results in a billable API call (like `Chat::send` or `Sorter::run_sorting_task`) must call `log_usage_turn`.
    *   **Provide a Clear Label**: When logging usage, provide a descriptive `label` (e.g., "convo with tools", "sort 50 items") to make the usage logs easy to analyze.

All future contributions should adhere to these standards.

***